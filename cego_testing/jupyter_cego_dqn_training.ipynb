{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import DQNAgent\n",
    "from rlcard.agents.random_agent import RandomAgent\n",
    "\n",
    "from rlcard.utils import (\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    "    get_device,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the arguments:\n",
    "* **_log_dir**: The directory to save the logs.\n",
    "* **_env_name**: The name of the environment.\n",
    "* **_seed**: The seed for the random number generator.\n",
    "* **_mlp_layer**: The size of the multi-layer perceptron.\n",
    "* **_num_eval_games**: The number of games to evaluate the agent.\n",
    "* **_num_eval_episodes**: The number of episodes to train the agent.\n",
    "* **_evaluate_every**: How often the agent is evaluated (interval of training episodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"_log_dir\": \"experiments/cego_dqn_result_testtest/\",\n",
    "    \"_env_name\": \"cego\",\n",
    "    \"_seed\": 10,\n",
    "    \"_mlp_layer\": [512, 512],\n",
    "    \"_num_eval_games\": 10000,\n",
    "    \"_num_episodes\": 500,\n",
    "    \"_evaluate_every\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(_log_dir, _env_name, _seed, _mlp_layer, _num_eval_games=10000, _num_episodes=1000, _evaluate_every=100):\n",
    "\n",
    "    # Check whether gpu is available\n",
    "    device = get_device()\n",
    "\n",
    "    set_seed(_seed)\n",
    "\n",
    "    # Make the environment with seed\n",
    "    env = rlcard.make(\n",
    "        _env_name,\n",
    "        config={\n",
    "            'seed': _seed,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # # this the DQN agent\n",
    "    dqn_agent = DQNAgent(\n",
    "        num_actions=env.num_actions,\n",
    "        state_shape=env.state_shape[0],\n",
    "        mlp_layers=_mlp_layer,\n",
    "        device=device,\n",
    "    )\n",
    "    random_agent1 = RandomAgent(num_actions=env.num_actions)\n",
    "    random_agent2 = RandomAgent(num_actions=env.num_actions)\n",
    "    random_agent3 = RandomAgent(num_actions=env.num_actions)\n",
    "\n",
    "    agents = [random_agent1, dqn_agent, random_agent2, random_agent3]\n",
    "\n",
    "    env.set_agents(agents)  # set agents to the environment\n",
    "\n",
    "    # Start training\n",
    "    with Logger(_log_dir) as logger:\n",
    "        for episode in range(_num_episodes):\n",
    "\n",
    "            # Generate data from the environment\n",
    "            trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "            # Reorganaize the data to be state, action, reward, next_state, done\n",
    "            trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "            # Feed transitions into agent memory, and train the agent\n",
    "            for ts in trajectories[0]:\n",
    "                dqn_agent.feed(ts)\n",
    "\n",
    "            # Evaluate the performance.\n",
    "            if episode % _evaluate_every == 0:\n",
    "                logger.log_performance(\n",
    "                    env.timestep,\n",
    "                    tournament(\n",
    "                        env,\n",
    "                        _num_eval_games,\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "        # Get the paths\n",
    "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plot_curve(csv_path, fig_path, \"DQN\")\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(_log_dir, 'model.pth')\n",
    "    torch.save(dqn_agent, save_path)\n",
    "    print('Model saved in', save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start The Training\n",
    "1. use cpu\n",
    "2. train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running on the CPU\n",
      "\n",
      "Logs saved in experiments/cego_dqn_result_testtest/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000007?line=0'>1</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000007?line=1'>2</a>\u001b[0m train(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs)\n",
      "\u001b[1;32m/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb Cell 6'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(_log_dir, _env_name, _seed, _mlp_layer, _num_eval_games, _num_episodes, _evaluate_every)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=44'>45</a>\u001b[0m     \u001b[39m# Evaluate the performance.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m _evaluate_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=46'>47</a>\u001b[0m         logger\u001b[39m.\u001b[39mlog_performance(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=47'>48</a>\u001b[0m             env\u001b[39m.\u001b[39mtimestep,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=48'>49</a>\u001b[0m             tournament(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=49'>50</a>\u001b[0m                 env,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=50'>51</a>\u001b[0m                 _num_eval_games,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=51'>52</a>\u001b[0m             )[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=52'>53</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=54'>55</a>\u001b[0m \u001b[39m# Get the paths\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philo/Dokumente/Studium/rlcard/cego_testing/cego_dqn_training.ipynb#ch0000005?line=55'>56</a>\u001b[0m csv_path, fig_path \u001b[39m=\u001b[39m logger\u001b[39m.\u001b[39mcsv_path, logger\u001b[39m.\u001b[39mfig_path\n",
      "File \u001b[0;32m~/Dokumente/Studium/rlcard/rlcard/utils/utils.py:210\u001b[0m, in \u001b[0;36mtournament\u001b[0;34m(env, num)\u001b[0m\n\u001b[1;32m    <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/utils/utils.py?line=207'>208</a>\u001b[0m counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/utils/utils.py?line=208'>209</a>\u001b[0m \u001b[39mwhile\u001b[39;00m counter \u001b[39m<\u001b[39m num:\n\u001b[0;32m--> <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/utils/utils.py?line=209'>210</a>\u001b[0m     _, _payoffs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrun(is_training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/utils/utils.py?line=210'>211</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_payoffs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/utils/utils.py?line=211'>212</a>\u001b[0m         \u001b[39mfor\u001b[39;00m _p \u001b[39min\u001b[39;00m _payoffs:\n",
      "File \u001b[0;32m~/Dokumente/Studium/rlcard/rlcard/envs/env.py:152\u001b[0m, in \u001b[0;36mEnv.run\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/env.py?line=148'>149</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents[player_id]\u001b[39m.\u001b[39mstep(state)\n\u001b[1;32m    <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/env.py?line=150'>151</a>\u001b[0m \u001b[39m# Environment steps\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/env.py?line=151'>152</a>\u001b[0m next_state, next_player_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/env.py?line=152'>153</a>\u001b[0m     action, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magents[player_id]\u001b[39m.\u001b[39;49muse_raw)\n\u001b[1;32m    <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/env.py?line=153'>154</a>\u001b[0m \u001b[39m# Save action\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/env.py?line=154'>155</a>\u001b[0m trajectories[player_id]\u001b[39m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/Dokumente/Studium/rlcard/rlcard/envs/env.py:88\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action, raw_action)\u001b[0m\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/env.py?line=84'>85</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_recorder\u001b[39m.\u001b[39mappend((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_player_id(), action))\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/env.py?line=85'>86</a>\u001b[0m next_state, player_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m---> <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/env.py?line=87'>88</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_state(next_state), player_id\n",
      "File \u001b[0;32m~/Dokumente/Studium/rlcard/rlcard/envs/cego.py:62\u001b[0m, in \u001b[0;36mCegoEnv._extract_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/cego.py?line=59'>60</a>\u001b[0m     trick_card_idx \u001b[39m=\u001b[39m [ACTION_SPACE[card] \u001b[39mfor\u001b[39;00m card \u001b[39min\u001b[39;00m state[\u001b[39m'\u001b[39m\u001b[39mtrick\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/cego.py?line=60'>61</a>\u001b[0m \u001b[39mif\u001b[39;00m state[\u001b[39m'\u001b[39m\u001b[39mplayed_tricks\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/cego.py?line=61'>62</a>\u001b[0m     played_cards_idx \u001b[39m=\u001b[39m get_tricks_played(state[\u001b[39m'\u001b[39;49m\u001b[39mplayed_tricks\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/cego.py?line=63'>64</a>\u001b[0m winner_idx \u001b[39m=\u001b[39m state[\u001b[39m'\u001b[39m\u001b[39mwinner\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/envs/cego.py?line=64'>65</a>\u001b[0m start_player_idx \u001b[39m=\u001b[39m state[\u001b[39m'\u001b[39m\u001b[39mstart_player\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Dokumente/Studium/rlcard/rlcard/games/cego/utils.py:50\u001b[0m, in \u001b[0;36mget_tricks_played\u001b[0;34m(tricks)\u001b[0m\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/games/cego/utils.py?line=47'>48</a>\u001b[0m card_idxs \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/games/cego/utils.py?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m trick \u001b[39min\u001b[39;00m tricks:\n\u001b[0;32m---> <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/games/cego/utils.py?line=49'>50</a>\u001b[0m     \u001b[39mfor\u001b[39;00m card \u001b[39min\u001b[39;00m trick:\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/games/cego/utils.py?line=50'>51</a>\u001b[0m         card_idxs\u001b[39m.\u001b[39mappend(ACTION_SPACE[\u001b[39mstr\u001b[39m(card)])\n\u001b[1;32m     <a href='file:///home/philo/Dokumente/Studium/rlcard/rlcard/games/cego/utils.py?line=52'>53</a>\u001b[0m \u001b[39mreturn\u001b[39;00m card_idxs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "train(**args)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f86c348c416147792ab5c695abcd41b8f3fbe6213677a915ef1193c06ddbb8a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
